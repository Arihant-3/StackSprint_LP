{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b781732e",
   "metadata": {},
   "source": [
    "# ‚ÅâÔ∏è **Day 16: BigQuery + API for Large Dataset Access**\n",
    "\n",
    "**Topic:** How to connect to BigQuery using Python and explore large-scale data with API-style queries.\n",
    "\n",
    "**Goal:** Learn to pull data efficiently from massive databases (e.g., millions of rows) using **BigQuery**, and optionally use it in Streamlit or other apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05941e36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c21eb",
   "metadata": {},
   "source": [
    "## üí° What is BigQuery?\n",
    "\n",
    "> BigQuery is a fully-managed, serverless cloud data warehouse by Google ‚Äî optimized for running fast SQL queries on massive datasets (think: TBs+).\n",
    "> \n",
    "\n",
    "### üî• Why Learn It?\n",
    "\n",
    "- Great for data analysts, ML engineers, and app developers.\n",
    "- Ideal for real-time analytics dashboards and large-scale joins.\n",
    "- Many **public datasets** are available for free."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be8477",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b0176",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup Instructions\n",
    "\n",
    "### Google Cloud BigQuery API Setup\n",
    "\n",
    "This guide will walk you through the process of setting up a Google Cloud project, enabling the BigQuery API, and creating a service account with a `credentials.json` file. This file is essential for your application to securely authenticate and interact with the BigQuery API.\n",
    "\n",
    "#### **Step 1: Set Up a Google Cloud Project**\n",
    "\n",
    "1. **Navigate to the Google Cloud Console:**\n",
    "Go to `https://console.cloud.google.com/`. You will be prompted to log in with your Google account.\n",
    "2. **Create a New Project:**\n",
    "At the top of the page, click the project selector dropdown (it usually shows the current project name or \"My First Project\").\n",
    "In the dialog that appears, click **New Project**.\n",
    "3. **Fill in Project Details:**\n",
    "    - **Project name:** Choose a descriptive name for your project (e.g., \"My-BigQuery-App\").\n",
    "    - **Location:** (Optional) Select an organization and a billing account if you have them. For personal use, you can usually leave these as default.\n",
    "    - Click **Create**.\n",
    "4. **Enable the BigQuery API:**\n",
    "Once your project is created, navigate to the **APIs & Services** dashboard.\n",
    "    - In the search bar, type `BigQuery API`.\n",
    "    - Click on **BigQuery API** from the results.\n",
    "    - Click the **Enable** button. This will activate the API for your new project.\n",
    "\n",
    "#### **Step 2: Create a Service Account**\n",
    "\n",
    "A service account is a special Google account that your application can use to make authorized API calls.\n",
    "\n",
    "1. **Go to IAM & Admin:**\n",
    "In the left-hand navigation menu, select **IAM & Admin > Service Accounts**.\n",
    "2. **Create a New Service Account:**\n",
    "Click the **Create Service Account** button at the top.\n",
    "    - **Service account name:** Give it a clear name (e.g., `bigquery-data-writer`).\n",
    "    - Click **Create and continue**.\n",
    "3. **Grant Permissions (Roles):**\n",
    "This is a critical step where you define what your application can do.\n",
    "    - In the \"Grant this service account access to project\" section, click the **Select a role** dropdown.\n",
    "    - Search for and select the **BigQuery Admin** role. This is a broad role that gives your account full control, which is fine for a learning project. For a production environment, you would typically use a more specific role like `BigQuery Data Editor` or `BigQuery Job User` to follow the principle of least privilege.\n",
    "    - Click **Done**.\n",
    "\n",
    "#### **Step 3: Download the Credentials File**\n",
    "\n",
    "1. **Manage Keys:**\n",
    "You will be redirected to the list of service accounts. Click on the one you just created.\n",
    "Navigate to the **Keys** tab and click **Add Key > Create new key**.\n",
    "2. **Choose JSON Format:**\n",
    "Select **JSON** as the key type.\n",
    "Click **Create**.\n",
    "3. **Download and Save:**\n",
    "Your browser will automatically download a JSON file. This file contains the private key and other credentials.\n",
    "**Rename this file to `credentials.json`** as you planned, and save it in a secure location within your project directory. This file should be treated like a password and should never be committed to a public version control system like GitHub.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Install Required Packages:\n",
    "\n",
    "```bash\n",
    "pip install google-cloud-bigquery\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7ded0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d79642",
   "metadata": {},
   "source": [
    "#### üß∞ Example: Query Public GitHub Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aec5797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agry/NGECore2: 34847 Python files\n",
      "ProjectSWGCore/NGECore2: 34847 Python files\n",
      "Azure/azure-sdk-for-python: 31035 Python files\n",
      "ryfeus/lambda-packs: 29849 Python files\n",
      "uberamd/NGECore2: 28728 Python files\n",
      "mancoast/CPythonPyc_test: 24034 Python files\n",
      "MalloyPower/parsing-python: 18772 Python files\n",
      "anhstudios/swganh: 16380 Python files\n",
      "obi-two/Rebelion: 16379 Python files\n",
      "kenshay/ImageScript: 15962 Python files\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Load service account credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(\"credentials.json\")\n",
    "\n",
    "# Initialize client\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Sample query (count Python files in GitHub repos)\n",
    "query = \"\"\"\n",
    "    SELECT\n",
    "      repo_name,\n",
    "      COUNT(*) AS file_count\n",
    "    FROM `bigquery-public-data.github_repos.files`\n",
    "    WHERE path LIKE '%.py'\n",
    "    GROUP BY repo_name\n",
    "    ORDER BY file_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Run query\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()\n",
    "\n",
    "# Display results\n",
    "for row in results:\n",
    "    print(f\"{row.repo_name}: {row.file_count} Python files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506e90d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e523b9e4",
   "metadata": {},
   "source": [
    "## üîê Important Notes:\n",
    "\n",
    "- Your query costs will be minimal on public datasets.\n",
    "- Avoid SELECT * on large datasets ‚Äî it costs more.\n",
    "- Always test with LIMIT first.\n",
    "- Store your credentials file securely (use `.gitignore`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2689ff9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe70a4",
   "metadata": {},
   "source": [
    "## üîç Other Interesting Public Datasets:\n",
    "\n",
    "| Dataset | Table Example |\n",
    "| --- | --- |\n",
    "| `bigquery-public-data.github_repos` | GitHub files and commits |\n",
    "| `bigquery-public-data.covid19_ecdc` | COVID-19 case data |\n",
    "| `bigquery-public-data.wikipedia` | Page views, edits |\n",
    "| `bigquery-public-data.crypto_ethereum` | Ethereum blockchain data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2315f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d15b9",
   "metadata": {},
   "source": [
    "## üìù Summary:\n",
    "\n",
    "- BigQuery is great for querying massive datasets quickly with SQL.\n",
    "- You accessed it using Python + service account key.\n",
    "- You learned to safely run cloud-based queries from your local script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stack_lp (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
